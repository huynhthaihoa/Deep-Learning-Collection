{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 68.55696105957031)\n",
      "(1, 64.40031433105469)\n",
      "(2, 57.4582405090332)\n",
      "(3, 68.0625228881836)\n",
      "(4, 67.96131896972656)\n",
      "(5, 67.4065933227539)\n",
      "(6, 67.52629089355469)\n",
      "(7, 64.49079132080078)\n",
      "(8, 31.04802131652832)\n",
      "(9, 27.771982192993164)\n",
      "(10, 66.30720520019531)\n",
      "(11, 61.11796951293945)\n",
      "(12, 59.83041763305664)\n",
      "(13, 16.875253677368164)\n",
      "(14, 14.834412574768066)\n",
      "(15, 63.62442398071289)\n",
      "(16, 53.64186096191406)\n",
      "(17, 51.711700439453125)\n",
      "(18, 65.10582733154297)\n",
      "(19, 59.95439147949219)\n",
      "(20, 45.11320495605469)\n",
      "(21, 42.68074417114258)\n",
      "(22, 40.06452941894531)\n",
      "(23, 12.281163215637207)\n",
      "(24, 51.818416595458984)\n",
      "(25, 32.959800720214844)\n",
      "(26, 47.32932662963867)\n",
      "(27, 12.875962257385254)\n",
      "(28, 11.506110191345215)\n",
      "(29, 40.70511245727539)\n",
      "(30, 55.15180969238281)\n",
      "(31, 36.384178161621094)\n",
      "(32, 5.563450336456299)\n",
      "(33, 4.731485366821289)\n",
      "(34, 3.8788135051727295)\n",
      "(35, 21.744544982910156)\n",
      "(36, 43.13826370239258)\n",
      "(37, 41.036827087402344)\n",
      "(38, 26.631256103515625)\n",
      "(39, 18.915164947509766)\n",
      "(40, 24.17453384399414)\n",
      "(41, 22.682395935058594)\n",
      "(42, 5.924004554748535)\n",
      "(43, 12.793107032775879)\n",
      "(44, 11.265641212463379)\n",
      "(45, 25.35838508605957)\n",
      "(46, 8.316976547241211)\n",
      "(47, 9.34359073638916)\n",
      "(48, 6.3707427978515625)\n",
      "(49, 18.57836151123047)\n",
      "(50, 9.802971839904785)\n",
      "(51, 4.787074089050293)\n",
      "(52, 7.430152893066406)\n",
      "(53, 11.951053619384766)\n",
      "(54, 4.6526384353637695)\n",
      "(55, 4.457189559936523)\n",
      "(56, 13.944796562194824)\n",
      "(57, 12.829875946044922)\n",
      "(58, 6.246438026428223)\n",
      "(59, 8.314807891845703)\n",
      "(60, 6.005230903625488)\n",
      "(61, 4.1611785888671875)\n",
      "(62, 2.683854103088379)\n",
      "(63, 1.9949036836624146)\n",
      "(64, 3.956789970397949)\n",
      "(65, 3.8643577098846436)\n",
      "(66, 5.213559150695801)\n",
      "(67, 4.70450496673584)\n",
      "(68, 4.135525703430176)\n",
      "(69, 2.677821159362793)\n",
      "(70, 3.4874045848846436)\n",
      "(71, 4.013214111328125)\n",
      "(72, 3.8714847564697266)\n",
      "(73, 3.1262691020965576)\n",
      "(74, 2.9015796184539795)\n",
      "(75, 2.5408952236175537)\n",
      "(76, 3.7164642810821533)\n",
      "(77, 1.8671462535858154)\n",
      "(78, 1.4853354692459106)\n",
      "(79, 1.7841973304748535)\n",
      "(80, 1.6563113927841187)\n",
      "(81, 1.6134264469146729)\n",
      "(82, 1.637150526046753)\n",
      "(83, 1.5477235317230225)\n",
      "(84, 2.615108013153076)\n",
      "(85, 2.4389235973358154)\n",
      "(86, 3.3148367404937744)\n",
      "(87, 1.9828495979309082)\n",
      "(88, 2.5575108528137207)\n",
      "(89, 1.115099310874939)\n",
      "(90, 1.967616081237793)\n",
      "(91, 0.9035945534706116)\n",
      "(92, 1.528825044631958)\n",
      "(93, 1.9212816953659058)\n",
      "(94, 1.6734600067138672)\n",
      "(95, 1.9662768840789795)\n",
      "(96, 1.2190967798233032)\n",
      "(97, 1.1121183633804321)\n",
      "(98, 0.9721309542655945)\n",
      "(99, 1.3496925830841064)\n",
      "(100, 1.275407314300537)\n",
      "(101, 1.1732516288757324)\n",
      "(102, 1.0556175708770752)\n",
      "(103, 0.8736497163772583)\n",
      "(104, 0.9875146746635437)\n",
      "(105, 0.779724657535553)\n",
      "(106, 1.0355879068374634)\n",
      "(107, 1.0262908935546875)\n",
      "(108, 0.9177124500274658)\n",
      "(109, 0.6687732338905334)\n",
      "(110, 0.6355648040771484)\n",
      "(111, 0.872856855392456)\n",
      "(112, 1.0000821352005005)\n",
      "(113, 0.8117831349372864)\n",
      "(114, 0.5724679827690125)\n",
      "(115, 0.5678190588951111)\n",
      "(116, 1.0041606426239014)\n",
      "(117, 0.9632062911987305)\n",
      "(118, 0.5397735834121704)\n",
      "(119, 0.7208158373832703)\n",
      "(120, 0.6970574259757996)\n",
      "(121, 0.6612656116485596)\n",
      "(122, 0.709017276763916)\n",
      "(123, 0.6068961024284363)\n",
      "(124, 0.645119845867157)\n",
      "(125, 0.6113700866699219)\n",
      "(126, 0.6768298149108887)\n",
      "(127, 0.9779959917068481)\n",
      "(128, 0.947910487651825)\n",
      "(129, 0.8819578886032104)\n",
      "(130, 0.5037627816200256)\n",
      "(131, 0.7381807565689087)\n",
      "(132, 0.6747440695762634)\n",
      "(133, 0.5401455760002136)\n",
      "(134, 0.5178136825561523)\n",
      "(135, 0.5221168398857117)\n",
      "(136, 0.5922502875328064)\n",
      "(137, 0.5246888399124146)\n",
      "(138, 0.5197998881340027)\n",
      "(139, 0.49833646416664124)\n",
      "(140, 0.4888276159763336)\n",
      "(141, 0.47929927706718445)\n",
      "(142, 0.46996524930000305)\n",
      "(143, 0.7739089131355286)\n",
      "(144, 0.4912509620189667)\n",
      "(145, 0.7105062007904053)\n",
      "(146, 0.46849894523620605)\n",
      "(147, 0.4655649662017822)\n",
      "(148, 0.5453749895095825)\n",
      "(149, 0.5545292496681213)\n",
      "(150, 0.4896514117717743)\n",
      "(151, 0.42733514308929443)\n",
      "(152, 0.5014117956161499)\n",
      "(153, 0.47792378067970276)\n",
      "(154, 0.46558523178100586)\n",
      "(155, 0.5120238661766052)\n",
      "(156, 0.4063217341899872)\n",
      "(157, 0.4029099643230438)\n",
      "(158, 0.5256522297859192)\n",
      "(159, 0.49766504764556885)\n",
      "(160, 0.5061463713645935)\n",
      "(161, 0.36976805329322815)\n",
      "(162, 0.361371785402298)\n",
      "(163, 0.35383865237236023)\n",
      "(164, 0.48000991344451904)\n",
      "(165, 0.4729548692703247)\n",
      "(166, 0.45626890659332275)\n",
      "(167, 0.3436278700828552)\n",
      "(168, 0.4444941282272339)\n",
      "(169, 0.44163429737091064)\n",
      "(170, 0.4329681098461151)\n",
      "(171, 0.4238564372062683)\n",
      "(172, 0.4540226459503174)\n",
      "(173, 0.33956506848335266)\n",
      "(174, 0.4038330912590027)\n",
      "(175, 0.4363686740398407)\n",
      "(176, 0.4052790105342865)\n",
      "(177, 0.3987938463687897)\n",
      "(178, 0.4027005434036255)\n",
      "(179, 0.4081977605819702)\n",
      "(180, 0.4103405475616455)\n",
      "(181, 0.3385487198829651)\n",
      "(182, 0.3983153998851776)\n",
      "(183, 0.3276411294937134)\n",
      "(184, 0.3211943209171295)\n",
      "(185, 0.3676176965236664)\n",
      "(186, 0.311101496219635)\n",
      "(187, 0.30751675367355347)\n",
      "(188, 0.3037410080432892)\n",
      "(189, 0.34307417273521423)\n",
      "(190, 0.4095078110694885)\n",
      "(191, 0.4383903443813324)\n",
      "(192, 0.43005260825157166)\n",
      "(193, 0.37473925948143005)\n",
      "(194, 0.3479633629322052)\n",
      "(195, 0.28907033801078796)\n",
      "(196, 0.3444131016731262)\n",
      "(197, 0.3588586747646332)\n",
      "(198, 0.3710741102695465)\n",
      "(199, 0.35806804895401)\n",
      "(200, 0.28172191977500916)\n",
      "(201, 0.356166273355484)\n",
      "(202, 0.3468308448791504)\n",
      "(203, 0.34101855754852295)\n",
      "(204, 0.33306947350502014)\n",
      "(205, 0.327303946018219)\n",
      "(206, 0.3318725824356079)\n",
      "(207, 0.27150821685791016)\n",
      "(208, 0.26712480187416077)\n",
      "(209, 0.35348501801490784)\n",
      "(210, 0.3028584420681)\n",
      "(211, 0.35047098994255066)\n",
      "(212, 0.3194188177585602)\n",
      "(213, 0.2888026237487793)\n",
      "(214, 0.3217729330062866)\n",
      "(215, 0.25554585456848145)\n",
      "(216, 0.2737066447734833)\n",
      "(217, 0.33965685963630676)\n",
      "(218, 0.3119201958179474)\n",
      "(219, 0.30841928720474243)\n",
      "(220, 0.26870688796043396)\n",
      "(221, 0.24156109988689423)\n",
      "(222, 0.2963334023952484)\n",
      "(223, 0.26518529653549194)\n",
      "(224, 0.2596729099750519)\n",
      "(225, 0.23210294544696808)\n",
      "(226, 0.2897563874721527)\n",
      "(227, 0.24181413650512695)\n",
      "(228, 0.28821009397506714)\n",
      "(229, 0.28541210293769836)\n",
      "(230, 0.3561027944087982)\n",
      "(231, 0.22277674078941345)\n",
      "(232, 0.22018134593963623)\n",
      "(233, 0.23635956645011902)\n",
      "(234, 0.2660958170890808)\n",
      "(235, 0.21164263784885406)\n",
      "(236, 0.35016292333602905)\n",
      "(237, 0.3432270884513855)\n",
      "(238, 0.33036449551582336)\n",
      "(239, 0.2609386444091797)\n",
      "(240, 0.21042726933956146)\n",
      "(241, 0.29339978098869324)\n",
      "(242, 0.25382956862449646)\n",
      "(243, 0.2702391445636749)\n",
      "(244, 0.27168363332748413)\n",
      "(245, 0.2542628049850464)\n",
      "(246, 0.2741631865501404)\n",
      "(247, 0.27158069610595703)\n",
      "(248, 0.26528438925743103)\n",
      "(249, 0.2634747326374054)\n",
      "(250, 0.21286015212535858)\n",
      "(251, 0.24849604070186615)\n",
      "(252, 0.24130694568157196)\n",
      "(253, 0.22907347977161407)\n",
      "(254, 0.24783077836036682)\n",
      "(255, 0.27959293127059937)\n",
      "(256, 0.2810845673084259)\n",
      "(257, 0.2553032636642456)\n",
      "(258, 0.25394392013549805)\n",
      "(259, 0.27616187930107117)\n",
      "(260, 0.2071346938610077)\n",
      "(261, 0.20431429147720337)\n",
      "(262, 0.19926172494888306)\n",
      "(263, 0.19370363652706146)\n",
      "(264, 0.24323329329490662)\n",
      "(265, 0.27779248356819153)\n",
      "(266, 0.24487744271755219)\n",
      "(267, 0.19182880222797394)\n",
      "(268, 0.2725074291229248)\n",
      "(269, 0.26599863171577454)\n",
      "(270, 0.18325014412403107)\n",
      "(271, 0.23508618772029877)\n",
      "(272, 0.17880971729755402)\n",
      "(273, 0.1965717375278473)\n",
      "(274, 0.24349476397037506)\n",
      "(275, 0.17337054014205933)\n",
      "(276, 0.23355308175086975)\n",
      "(277, 0.23031426966190338)\n",
      "(278, 0.24171490967273712)\n",
      "(279, 0.24033179879188538)\n",
      "(280, 0.21710331737995148)\n",
      "(281, 0.17005561292171478)\n",
      "(282, 0.23412609100341797)\n",
      "(283, 0.21937750279903412)\n",
      "(284, 0.22837063670158386)\n",
      "(285, 0.22383251786231995)\n",
      "(286, 0.17224064469337463)\n",
      "(287, 0.21675363183021545)\n",
      "(288, 0.21248140931129456)\n",
      "(289, 0.1735827475786209)\n",
      "(290, 0.17269858717918396)\n",
      "(291, 0.19940997660160065)\n",
      "(292, 0.1930703967809677)\n",
      "(293, 0.22116978466510773)\n",
      "(294, 0.21441172063350677)\n",
      "(295, 0.1582336127758026)\n",
      "(296, 0.17002302408218384)\n",
      "(297, 0.21833032369613647)\n",
      "(298, 0.21564117074012756)\n",
      "(299, 0.16036421060562134)\n",
      "(300, 0.2355506271123886)\n",
      "(301, 0.15494127571582794)\n",
      "(302, 0.1504264622926712)\n",
      "(303, 0.1447974145412445)\n",
      "(304, 0.1391996145248413)\n",
      "(305, 0.24922794103622437)\n",
      "(306, 0.1317485272884369)\n",
      "(307, 0.15501748025417328)\n",
      "(308, 0.1543901115655899)\n",
      "(309, 0.2531008720397949)\n",
      "(310, 0.23143082857131958)\n",
      "(311, 0.2259288728237152)\n",
      "(312, 0.1236080452799797)\n",
      "(313, 0.1383976936340332)\n",
      "(314, 0.13590985536575317)\n",
      "(315, 0.23786720633506775)\n",
      "(316, 0.1342218518257141)\n",
      "(317, 0.128822460770607)\n",
      "(318, 0.22927974164485931)\n",
      "(319, 0.13596434891223907)\n",
      "(320, 0.1356055587530136)\n",
      "(321, 0.20218972861766815)\n",
      "(322, 0.13194669783115387)\n",
      "(323, 0.20023377239704132)\n",
      "(324, 0.22316579520702362)\n",
      "(325, 0.12576039135456085)\n",
      "(326, 0.12319409847259521)\n",
      "(327, 0.22161145508289337)\n",
      "(328, 0.21872025728225708)\n",
      "(329, 0.12386011332273483)\n",
      "(330, 0.1251932978630066)\n",
      "(331, 0.12441207468509674)\n",
      "(332, 0.12163221836090088)\n",
      "(333, 0.207656130194664)\n",
      "(334, 0.11560095101594925)\n",
      "(335, 0.1126704216003418)\n",
      "(336, 0.20972943305969238)\n",
      "(337, 0.20940743386745453)\n",
      "(338, 0.21648453176021576)\n",
      "(339, 0.2134958952665329)\n",
      "(340, 0.12404625117778778)\n",
      "(341, 0.19841043651103973)\n",
      "(342, 0.11608558893203735)\n",
      "(343, 0.18287567794322968)\n",
      "(344, 0.11926289647817612)\n",
      "(345, 0.11822305619716644)\n",
      "(346, 0.12639136612415314)\n",
      "(347, 0.11333231627941132)\n",
      "(348, 0.16370822489261627)\n",
      "(349, 0.21516446769237518)\n",
      "(350, 0.1095762625336647)\n",
      "(351, 0.16164565086364746)\n",
      "(352, 0.1265915483236313)\n",
      "(353, 0.21453697979450226)\n",
      "(354, 0.12366698682308197)\n",
      "(355, 0.15764771401882172)\n",
      "(356, 0.20774151384830475)\n",
      "(357, 0.15566101670265198)\n",
      "(358, 0.2001258283853531)\n",
      "(359, 0.15322664380073547)\n",
      "(360, 0.12454575300216675)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361, 0.12505511939525604)\n",
      "(362, 0.12213917821645737)\n",
      "(363, 0.1546889692544937)\n",
      "(364, 0.18771693110466003)\n",
      "(365, 0.11050982773303986)\n",
      "(366, 0.18495656549930573)\n",
      "(367, 0.10939125716686249)\n",
      "(368, 0.1789781153202057)\n",
      "(369, 0.16344432532787323)\n",
      "(370, 0.1720968782901764)\n",
      "(371, 0.11583121865987778)\n",
      "(372, 0.16257531940937042)\n",
      "(373, 0.10569147765636444)\n",
      "(374, 0.16535578668117523)\n",
      "(375, 0.155373215675354)\n",
      "(376, 0.16380517184734344)\n",
      "(377, 0.16193357110023499)\n",
      "(378, 0.13198710978031158)\n",
      "(379, 0.1080806702375412)\n",
      "(380, 0.10643331706523895)\n",
      "(381, 0.15422247350215912)\n",
      "(382, 0.10351108014583588)\n",
      "(383, 0.159107968211174)\n",
      "(384, 0.10121788084506989)\n",
      "(385, 0.15437807142734528)\n",
      "(386, 0.15408433973789215)\n",
      "(387, 0.15222929418087006)\n",
      "(388, 0.09942464530467987)\n",
      "(389, 0.12104000151157379)\n",
      "(390, 0.1467503309249878)\n",
      "(391, 0.1176227480173111)\n",
      "(392, 0.1447749137878418)\n",
      "(393, 0.14344052970409393)\n",
      "(394, 0.16800083220005035)\n",
      "(395, 0.13989242911338806)\n",
      "(396, 0.10323711484670639)\n",
      "(397, 0.1639036238193512)\n",
      "(398, 0.11775948107242584)\n",
      "(399, 0.10277019441127777)\n",
      "(400, 0.1395304799079895)\n",
      "(401, 0.11336822062730789)\n",
      "(402, 0.14085538685321808)\n",
      "(403, 0.10773314535617828)\n",
      "(404, 0.1045645922422409)\n",
      "(405, 0.09576775878667831)\n",
      "(406, 0.09745685756206512)\n",
      "(407, 0.14981737732887268)\n",
      "(408, 0.15139585733413696)\n",
      "(409, 0.16841773688793182)\n",
      "(410, 0.1648685336112976)\n",
      "(411, 0.09006407111883163)\n",
      "(412, 0.08898231387138367)\n",
      "(413, 0.15127642452716827)\n",
      "(414, 0.15033632516860962)\n",
      "(415, 0.08939759433269501)\n",
      "(416, 0.13902264833450317)\n",
      "(417, 0.13609160482883453)\n",
      "(418, 0.131153404712677)\n",
      "(419, 0.1452413648366928)\n",
      "(420, 0.1441461145877838)\n",
      "(421, 0.09547346085309982)\n",
      "(422, 0.13794708251953125)\n",
      "(423, 0.09373621642589569)\n",
      "(424, 0.12661921977996826)\n",
      "(425, 0.12073299288749695)\n",
      "(426, 0.08634498715400696)\n",
      "(427, 0.08436927944421768)\n",
      "(428, 0.10239376127719879)\n",
      "(429, 0.13720491528511047)\n",
      "(430, 0.07629312574863434)\n",
      "(431, 0.13233499228954315)\n",
      "(432, 0.12638579308986664)\n",
      "(433, 0.09170448780059814)\n",
      "(434, 0.11654886603355408)\n",
      "(435, 0.1127137690782547)\n",
      "(436, 0.15267729759216309)\n",
      "(437, 0.1534366011619568)\n",
      "(438, 0.10242568701505661)\n",
      "(439, 0.08859740197658539)\n",
      "(440, 0.0863703265786171)\n",
      "(441, 0.10885582864284515)\n",
      "(442, 0.07811437547206879)\n",
      "(443, 0.13710792362689972)\n",
      "(444, 0.07927151024341583)\n",
      "(445, 0.13289040327072144)\n",
      "(446, 0.12434195727109909)\n",
      "(447, 0.12703195214271545)\n",
      "(448, 0.1240917220711708)\n",
      "(449, 0.1061406284570694)\n",
      "(450, 0.11289721727371216)\n",
      "(451, 0.09960922598838806)\n",
      "(452, 0.12546002864837646)\n",
      "(453, 0.1273072063922882)\n",
      "(454, 0.10553498566150665)\n",
      "(455, 0.08074663579463959)\n",
      "(456, 0.10914693027734756)\n",
      "(457, 0.09237901866436005)\n",
      "(458, 0.07316620647907257)\n",
      "(459, 0.0843999981880188)\n",
      "(460, 0.12285209447145462)\n",
      "(461, 0.12292888015508652)\n",
      "(462, 0.12158036977052689)\n",
      "(463, 0.11919305473566055)\n",
      "(464, 0.11610429733991623)\n",
      "(465, 0.10542454570531845)\n",
      "(466, 0.11009935289621353)\n",
      "(467, 0.1450529247522354)\n",
      "(468, 0.08189161866903305)\n",
      "(469, 0.0779481828212738)\n",
      "(470, 0.08704110234975815)\n",
      "(471, 0.08935488760471344)\n",
      "(472, 0.11014942079782486)\n",
      "(473, 0.11121664196252823)\n",
      "(474, 0.07844926416873932)\n",
      "(475, 0.10869673639535904)\n",
      "(476, 0.08681631088256836)\n",
      "(477, 0.12598153948783875)\n",
      "(478, 0.08129295706748962)\n",
      "(479, 0.0795239582657814)\n",
      "(480, 0.0775766670703888)\n",
      "(481, 0.07117141783237457)\n",
      "(482, 0.07070926576852798)\n",
      "(483, 0.07033644616603851)\n",
      "(484, 0.1194288432598114)\n",
      "(485, 0.13345208764076233)\n",
      "(486, 0.1284346580505371)\n",
      "(487, 0.12085364758968353)\n",
      "(488, 0.12480311095714569)\n",
      "(489, 0.08764630556106567)\n",
      "(490, 0.10486908257007599)\n",
      "(491, 0.07703143358230591)\n",
      "(492, 0.09862735867500305)\n",
      "(493, 0.0956120565533638)\n",
      "(494, 0.09047470986843109)\n",
      "(495, 0.12990880012512207)\n",
      "(496, 0.0889328271150589)\n",
      "(497, 0.0938691794872284)\n",
      "(498, 0.12585215270519257)\n",
      "(499, 0.08975556492805481)\n"
     ]
    }
   ],
   "source": [
    "# By Justin Johnson https://github.com/jcjohnson/pytorch-examples/blob/master/nn/dynamic_net.py\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\"\"\"\n",
    "To showcase the power of PyTorch dynamic graphs, we will implement a very strange\n",
    "model: a fully-connected ReLU network that on each forward pass randomly chooses\n",
    "a number between 1 and 4 and has that many hidden layers, reusing the same\n",
    "weights multiple times to compute the innermost hidden layers.\n",
    "\"\"\"\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we construct three nn.Linear instances that we will use\n",
    "    in the forward pass.\n",
    "    \"\"\"\n",
    "    super(DynamicNet, self).__init__()\n",
    "    self.input_linear = torch.nn.Linear(D_in, H)\n",
    "    self.middle_linear = torch.nn.Linear(H, H)\n",
    "    self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x, verbose = False):\n",
    "    \"\"\"\n",
    "    For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "    and reuse the middle_linear Module that many times to compute hidden layer\n",
    "    representations.\n",
    "    Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "    Python control-flow operators like loops or conditional statements when\n",
    "    defining the forward pass of the model.\n",
    "    Here we also see that it is perfectly safe to reuse the same Module many\n",
    "    times when defining a computational graph. This is a big improvement from Lua\n",
    "    Torch, where each Module could be used only once.\n",
    "    \"\"\"\n",
    "    h_relu = self.input_linear(x).clamp(min=0)\n",
    "    n_layers = random.randint(0, 3)\n",
    "    if verbose:\n",
    "        print(\"The number of layers for this run is\", n_layers)\n",
    "        # print(h_relu)\n",
    "    for _ in range(n_layers):\n",
    "        h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        if verbose:\n",
    "            pass\n",
    "            # print(h_relu)\n",
    "    y_pred = self.output_linear(h_relu)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10, 1\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss\n",
    "  loss = criterion(y_pred, y)\n",
    "  print(t, loss.data[0])\n",
    "\n",
    "  # Zero gradients, perform a backward pass, and update the weights.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7784\n",
       "-0.5659\n",
       "-1.1524\n",
       "-0.6715\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7171\n",
       "-0.5757\n",
       "-1.1496\n",
       "-0.6636\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[1:5] # another run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7219\n",
       "-0.6037\n",
       "-1.1652\n",
       "-0.7280\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks consistent! Let's now try to see what's happening inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7171\n",
       "-0.5757\n",
       "-1.1496\n",
       "-0.6636\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7784\n",
       "-0.5659\n",
       "-1.1524\n",
       "-0.6715\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7219\n",
       "-0.6037\n",
       "-1.1652\n",
       "-0.7280\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7171\n",
       "-0.5757\n",
       "-1.1496\n",
       "-0.6636\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of layers for this run is', 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7171\n",
       "-0.5757\n",
       "-1.1496\n",
       "-0.6636\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, verbose = True)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.7307\n",
       "-0.5801\n",
       "-1.1544\n",
       "-0.6970\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
